# -*- coding: utf-8 -*-
"""ML_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YhJOuSShjdqzU9k7atYdV3iXk-WtfUP5

# Importing dataset
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb

from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.svm import LinearSVC
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score
from sklearn import metrics
from sklearn.metrics import roc_curve, auc, roc_auc_score
from sklearn.model_selection import GridSearchCV
from sklearn import tree



# Importing dataset
url = 'https://raw.githubusercontent.com/Amaniiitd/Fetal-Health-classification/main/Dataset/fetal_health.csv'
df = pd.read_csv(url)
df.head()

"""# EDA"""

# printing dataset discription before preprocessing
print(df.info())
df.describe().T

# Count the missing and null values for dataset fetal healt.
miss_values = df.columns[df.isnull().any()]
print(f"Missing values:\n{df[miss_values].isnull().sum()}")

null_values = df.columns[df.isna().any()]
print(f"Null values:\n{df[null_values].isna().sum()}")

#Removing duplicates
dup_df=df.copy()
dup_df.drop_duplicates(inplace=True)
print('Total number of duplicate entries =', df.shape[0] - dup_df.shape[0])
df=dup_df

# Printing unique output classes
print('Uniques output classes -',df.fetal_health.unique().astype(int),'\n\n')


# Bar graph of output distribution
plt.figure(figsize=(8,5))
plt.title("Fetal health output distribution")
plt.xlabel("Fetal health")
plt.ylabel("Number of cases")
sb.countplot(data= df, x="fetal_health")
plt.show()


# Pie chart of output distribution
print(df[df.fetal_health==1.0].shape[0], df[df.fetal_health==2.0].shape[0], df[df.fetal_health==3.0].shape[0])
plt.figure(figsize=(6,6))
plt.title("Fetal health output distribution")
plt.pie([df[df.fetal_health==1.0].shape[0], df[df.fetal_health==2.0].shape[0], df[df.fetal_health==3.0].shape[0]], labels=["Normal", "Suspect", "Pathological"], autopct="%1.0f%%")
plt.show()

# Histograms of features
data_hist_plot = df.hist(figsize = (20,20), color = "#48a8ab")

# Plotting scatter matrix to see behavius of each feature
from pandas.plotting import scatter_matrix
scatterMatrix = scatter_matrix(df,figsize=(50, 50), color = "#48a8ab")
plt.show()

# Making correlation heatmap
plt.figure(figsize=(20, 10))
dataplot = sb.heatmap(df.corr(), cmap='PiYG', annot=True)
plt.show()

# Fetures in decresing order of there correlation with output
df.corr().fetal_health.sort_values(ascending=False)

"""# Feature Selection"""

from sklearn.feature_selection import SelectKBest #Feature Selector
from sklearn.feature_selection import f_classif #F-ratio statistic for categorical values

# Feature Selection
X=df.drop(["fetal_health"],axis=1)
y=df["fetal_health"]
feature_selector = SelectKBest(score_func=f_classif, k='all')
fit = feature_selector.fit(X,y)
dfscores = pd.DataFrame(fit.scores_)
dfcolumns = pd.DataFrame(X.columns)

# concat two dataframes for better visualization 
featureScores = pd.concat([dfcolumns,dfscores],axis=1)
featureScores.columns = ['Feature','Score']  # naming the dataframe columns

# Plot the feature scores
fig, ax=plt.subplots(figsize=(7,7))
plot=sb.barplot(data=featureScores, x='Score', y='Feature')
plot

# Selection method
selectedFeatures=featureScores[featureScores['Score']>=50]#Selects features that scored more than 50
selectedFeatures=list(selectedFeatures['Feature'])
selectedFeatures.append('fetal_health')

# Updating the dataframe
df=df[selectedFeatures]
print('Dataset dimensions after feature selection -',df.shape)

df.head()

# Splitting dataset into input and output i.e X and y
X=df.drop(["fetal_health"],axis=1)
y=df["fetal_health"]

# Making boxen plot of features before feature scalling
plt.figure(figsize=(20, 10))
sb.boxenplot(data = X)
plt.xticks(rotation=90)
plt.show()

def feature_standardize(X):
    #calculating the mean
    mu = np.mean(X, axis = 0)  
    
    #calculating the standard deviation
    sigma = np.std(X, axis= 0, ddof = 1)  # Standard deviation
    
    #Normalising the dataset
    X_norm = (X - mu)/sigma

    return X_norm
    
X=feature_standardize(X) # Scaling the features
print(X.head())

# Making boxen plot of features after feature scalling
plt.figure(figsize=(20, 10))
sb.boxenplot(data = X)
plt.xticks(rotation=90)
plt.show()

"""# Splitting Data"""

from sklearn.model_selection import train_test_split  

# Printing initial dimensions of dataset
print('Initial shape of dataset =',X.shape)
print(X.shape)

# Spliting test and training sets
X_train, X_test, Y_train,Y_test = train_test_split(X,y,test_size=0.3,random_state=42,stratify=y)

# Priniting dataset dimensions after split
print('\nAfter split,\nX_train =',X_train.shape, ', Y_train =', Y_train.shape, ', X_test =', X_test.shape, ', Y_test =', Y_test.shape)

"""# Applying models"""

# Training and Testing Logistic Regression
lr = LogisticRegression()
lr.fit(X_train,Y_train)
prediction_lr = lr.predict(X_test)

#Confusion Matrix 
print("Confusion Matrix Logistic Regression")
confmatrix_lr = metrics.confusion_matrix(Y_test,prediction_lr)
print("Accuracy Model LogisticRegression:",metrics.accuracy_score(Y_test, prediction_lr))
print(metrics.classification_report(Y_test, prediction_lr, digits=3))
sb.heatmap(pd.DataFrame(confmatrix_lr), annot=True, cmap="YlGnBu" ,fmt='g')

plt.tight_layout()
plt.title('Confusion matrix for LogisticRegression ', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')
plt.xticks(np.arange(3)+0.5,("Normal","Suspect","Pathological"),va='center')
plt.yticks(np.arange(3)+0.5,("Normal","Suspect","Pathological"),va='center')
plt.show()

from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score

#Plotting ROC Curve and Printing AUC Score for Logistic Regression
predict_prob = lr.predict_proba(X_test)

def separate(prediction):
    prediction_1vall = []
    prediction_2vall = []
    prediction_3vall = []
    for i in range(len(prediction)):
        if prediction[i]==1:
            prediction_1vall.append(1)
            prediction_2vall.append(0)
            prediction_3vall.append(0)
        if prediction[i]==2:
            prediction_1vall.append(0)
            prediction_2vall.append(1)
            prediction_3vall.append(0)
        if prediction[i]==3:
            prediction_1vall.append(0)
            prediction_2vall.append(0)
            prediction_3vall.append(1)
    return (prediction_1vall,prediction_2vall,prediction_3vall)

fpr = {}
tpr = {}   
Y_test_1, Y_test_2, Y_test_3 = separate(Y_test.to_list())
#Since There are 3 classes we will plot roc curve for each class vs all other classes

fpr[1],tpr[1],_ = roc_curve(Y_test_1,predict_prob[:,0], pos_label=1)
fpr[2],tpr[2],_ = roc_curve(Y_test_2,predict_prob[:,1], pos_label=1)
fpr[3],tpr[3],_ = roc_curve(Y_test_3,predict_prob[:,2], pos_label=1)
# print(_)

plt.plot(fpr[1],tpr[1],linestyle = '--',label='1 vs All',color='green')
plt.plot(fpr[2],tpr[2],linestyle = '--',label='2 vs All',color='orange')
plt.plot(fpr[3],tpr[3],linestyle = '--',label='3 vs All',color='red')
plt.plot([0,1],[0,1],linestyle='--',color='blue')
plt.title("ROC Curve Logistic Regression")
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.legend(loc='best')
plt.show()

auc_score = metrics.roc_auc_score(Y_test,predict_prob,multi_class='ovr',average='weighted')
print("AUC for Logistic Regression Model",auc_score) #AUC Score of LR Model

#Gaussian Bayes Model
bayes_model = GaussianNB()
bayes_model.fit(X_train,Y_train)
prediction_bayes = bayes_model.predict(X_test)
# print("Confusion Matrix GaussianNB")
confmatrix_bayes = metrics.confusion_matrix(Y_test,prediction_bayes)
print(confmatrix_bayes)

print("Accuracy Model GaussianNB:",metrics.accuracy_score(Y_test, prediction_bayes))
print("Classification Report for Bayes Classifier Model\n",metrics.classification_report(Y_test, prediction_bayes, digits=3))
sb.heatmap(pd.DataFrame(confmatrix_bayes), annot=True, cmap="YlGnBu" ,fmt='g')
plt.tight_layout()
plt.title('Confusion matrix GaussianNB')
plt.ylabel('Actual label')
plt.xlabel('Predicted label')
plt.xticks(np.arange(3)+0.5,("Normal","Suspect","Pathological"),va='center')
plt.yticks(np.arange(3)+0.5,("Normal","Suspect","Pathological"),va='center')
plt.show()

from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score

predict_prob = bayes_model.predict_proba(X_test)


def separate(prediction):
    prediction_1vall = []
    prediction_2vall = []
    prediction_3vall = []
    for i in range(len(prediction)):
        if prediction[i]==1:
            prediction_1vall.append(1)
            prediction_2vall.append(0)
            prediction_3vall.append(0)
        if prediction[i]==2:
            prediction_1vall.append(0)
            prediction_2vall.append(1)
            prediction_3vall.append(0)
        if prediction[i]==3:
            prediction_1vall.append(0)
            prediction_2vall.append(0)
            prediction_3vall.append(1)
    return (prediction_1vall,prediction_2vall,prediction_3vall)

fpr = {}
tpr = {}   
Y_test_1, Y_test_2, Y_test_3 = separate(Y_test.to_list())
# print(Y_test_1)
# print(Y_test_2)
# print(Y_test_3)
fpr[1],tpr[1],_ = roc_curve(Y_test_1,predict_prob[:,0], pos_label=1)
fpr[2],tpr[2],_ = roc_curve(Y_test_2,predict_prob[:,1], pos_label=1)
fpr[3],tpr[3],_ = roc_curve(Y_test_3,predict_prob[:,2], pos_label=1)
# print(_)

plt.plot(fpr[1],tpr[1],linestyle = '--',label='1 vs All',color='green')
plt.plot(fpr[2],tpr[2],linestyle = '--',label='2 vs All',color='orange')
plt.plot(fpr[3],tpr[3],linestyle = '--',label='3 vs All',color='red')
plt.plot([0,1],[0,1],linestyle='--',color='blue')
plt.title("ROC Curve Bayes Model")
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.legend(loc='best')
plt.show()

auc_score = metrics.roc_auc_score(Y_test,predict_prob,multi_class='ovr',average='weighted')
print("AUC for Bayes Model",auc_score)

#Decision Tree Classifier with criterion set to entropy
dtc = DecisionTreeClassifier(criterion='entropy',max_depth=14,random_state=24)
dtc.fit(X_train,Y_train)
prediction = dtc.predict(X_test)
# print(prediction)
confmatrix = metrics.confusion_matrix(Y_test,prediction)
# print(confmatrix)
accuracy = metrics.accuracy_score(Y_test,prediction)
print("Accuracy of DTC Model:",accuracy)

classification_rep = metrics.classification_report(Y_test,prediction,digits=3)
print("Classification Report of DTC Model:\n",classification_rep)
print(dtc.get_depth())

sb.heatmap(pd.DataFrame(confmatrix), annot=True, cmap="YlGnBu" ,fmt='g')
plt.tight_layout()
plt.title("DTC Confusion Matrix")
plt.ylabel("Actual Class")
plt.xlabel("Predicted Class")
plt.xticks(np.arange(3)+0.5,("Normal","Suspect","Pathological"),va='center')
plt.yticks(np.arange(3)+0.5,("Normal","Suspect","Pathological"),va='center')
plt.show()

from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score

predict_prob = dtc.predict_proba(X_test)

def separate(prediction):
    prediction_1vall = []
    prediction_2vall = []
    prediction_3vall = []
    for i in range(len(prediction)):
        if prediction[i]==1:
            prediction_1vall.append(1)
            prediction_2vall.append(0)
            prediction_3vall.append(0)
        if prediction[i]==2:
            prediction_1vall.append(0)
            prediction_2vall.append(1)
            prediction_3vall.append(0)
        if prediction[i]==3:
            prediction_1vall.append(0)
            prediction_2vall.append(0)
            prediction_3vall.append(1)
    return (prediction_1vall,prediction_2vall,prediction_3vall)

fpr = {}
tpr = {}   
Y_test_1, Y_test_2, Y_test_3 = separate(Y_test.to_list())
# print(Y_test_1)
# print(Y_test_2)
# print(Y_test_3)
fpr[1],tpr[1],_ = roc_curve(Y_test_1,predict_prob[:,0], pos_label=1)
fpr[2],tpr[2],_ = roc_curve(Y_test_2,predict_prob[:,1], pos_label=1)
fpr[3],tpr[3],_ = roc_curve(Y_test_3,predict_prob[:,2], pos_label=1)
# print(_)

plt.plot(fpr[1],tpr[1],linestyle = '--',label='1 vs All',color='green')
plt.plot(fpr[2],tpr[2],linestyle = '--',label='2 vs All',color='orange')
plt.plot(fpr[3],tpr[3],linestyle = '--',label='3 vs All',color='red')
plt.plot([0,1],[0,1],linestyle='--',color='blue')
plt.title("ROC Curve for DTC Model")
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.legend(loc='best')
plt.show()

auc_score = metrics.roc_auc_score(Y_test,predict_prob,multi_class='ovr',average='weighted')
print("AUC for DTC Model",auc_score)

#Visualisation of the decision tree
features = X.columns
features = features.to_list()
plt.figure(figsize=(80,50),dpi=100)
tree.plot_tree(dtc,feature_names=features,class_names=["Normal","Suspect","Pathological"],fontsize=15,filled=True)
plt.savefig("DecisionTree.png")

#Analysis of model on different depths to reduce overfitting
depths = []
accuracies = []
recall_1 = []
recall_2 = []
recall_3 = []
# print("Testing Set")
for i in range(1,20):
    dtc_depth_i = DecisionTreeClassifier(criterion='entropy',max_depth=i)
    dtc_depth_i.fit(X_train,Y_train)
    prediction_i = dtc_depth_i.predict(X_test)
    accuracy_i = metrics.accuracy_score(Y_test,prediction_i)
    depths.append(i)
    accuracies.append(accuracy_i)
    predict_prob = dtc_depth_i.predict_proba(X_test)
    classification_rep = metrics.classification_report(Y_test,prediction_i,output_dict=True,zero_division=0)
    # print(classification_rep)
    recall_1.append(classification_rep['1.0']['recall'])
    recall_2.append(classification_rep['2.0']['recall'])
    recall_3.append(classification_rep['3.0']['recall'])
    
    # print("Accuracy of DTC Model with depth = %d:"%i,accuracy_i)
    # print("Recall Class 2 of DTC Model with depth = %d:"%i,recall_2[i-1])
    # print("Recall Class 3 of DTC Model with depth = %d:"%i,recall_3[i-1])
    # classification_rep = metrics.classification_report(Y_test,prediction_i,digits=3)
    # print("Classification Report of DTC Model with depth = %d:\n"%i,classification_rep)
    


fig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2)
plt.figure(figsize=(100,100))
fig.tight_layout()
accuracies = np.array(accuracies)
errors = 1 - accuracies
ax1.plot(depths,accuracies,label='Testing Set Accuracy')
ax1.set_ylabel("Accuracy of Model")
ax1.set_xlabel("Depth of DT")
# ax1.set_ylim((0.6,1))

ax2.plot(depths,recall_1)
ax2.set_ylabel("Recall of Class 1")
ax2.set_xlabel("Depth of DT")
# ax2.set_ylim((0.6,1))

ax3.plot(depths,recall_2)
ax3.set_ylabel("Recall of Class 2")
ax3.set_xlabel("Depth of DT")
# ax3.set_ylim((0.6,1))

ax4.plot(depths,recall_3)
ax4.set_ylabel("Recall of Class 3")
ax4.set_xlabel("Depth of DT")
# ax4.set_ylim((0.6,1))

depths = []
accuracies = []
recall_1 = []
recall_2 = []
recall_3 = []
# print("Training Set")
for i in range(1,20):
    dtc_depth_i = DecisionTreeClassifier(criterion='entropy',max_depth=i)
    dtc_depth_i.fit(X_train,Y_train)
    prediction_i = dtc_depth_i.predict(X_train)
    accuracy_i = metrics.accuracy_score(Y_train,prediction_i)
    depths.append(i)
    accuracies.append(accuracy_i)
    classification_rep = metrics.classification_report(Y_train,prediction_i,output_dict=True,zero_division=0)
    predict_prob = dtc_depth_i.predict_proba(X_test)
    recall_1.append(classification_rep['1.0']['recall'])
    recall_2.append(classification_rep['2.0']['recall'])
    recall_3.append(classification_rep['3.0']['recall'])
    # print("Accuracy of DTC Model with depth = %d:"%i,accuracy_i)
    # classification_rep = metrics.classification_report(Y_train,prediction_i,digits=3)
    # print("Classification Report of DTC Model with depth = %d:\n"%i,classification_rep)
    


# fig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2)
# plt.figure(figsize=(100,100))
# fig.tight_layout()
accuracies = np.array(accuracies)
errors = 1 - accuracies
ax1.plot(depths,accuracies,label='Training Set Accuracy')
ax1.set_ylabel("Accuracy of Model")
ax1.set_xlabel("Depth of DT")
# ax1.set_ylim((0.6,1))

ax2.plot(depths,recall_1)
ax2.set_ylabel("Recall of Class 1")
ax2.set_xlabel("Depth of DT")
# ax2.set_ylim((0.6,1))

ax3.plot(depths,recall_2)
ax3.set_ylabel("Recall of Class 2")
ax3.set_xlabel("Depth of DT")
# ax3.set_ylim((0.6,1))

ax4.plot(depths,recall_3)
ax4.set_ylabel("Recall of Class 3")
ax4.set_xlabel("Depth of DT")
# ax4.set_ylim((0.6,1))

# plt.legend(['Orange : Testing','Blue: Training'])
ax1.legend(loc='best')

#Random Forest Classifier Model
rfc= RandomForestClassifier(criterion='entropy',random_state=42)
rfc.fit(X_train,Y_train)
prediction_rfc = rfc.predict(X_test)
# print("Confusion Matrix RFC")
confmatrix_rfc = metrics.confusion_matrix(Y_test,prediction_rfc)
print("The number of decision trees in forest",rfc.n_estimators)

# plt.plot(X_test,prediction_rfc,color='black',linewidth=3)
# plt.title('RFC plot')
# plt.show()

print("Accuracy Model RFC:",metrics.accuracy_score(Y_test, prediction_rfc))
print("Classification Report of RFC Model\n",metrics.classification_report(Y_test, prediction_rfc, digits=3))
sb.heatmap(pd.DataFrame(confmatrix_rfc), annot=True, cmap="YlGnBu" ,fmt='g')
plt.tight_layout()
plt.title('Confusion matrix RFC')
plt.ylabel('Actual label')
plt.xlabel('Predicted label')
plt.xticks(np.arange(3)+0.5,("Normal","Suspect","Pathological"),va='center')
plt.yticks(np.arange(3)+0.5,("Normal","Suspect","Pathological"),va='center')
plt.show()

#Running Grid Search on Random Forest to get the best model
estimators = [i for i in range(1,100,10)]
para = {'n_estimators':estimators}
rfc_best = RandomForestClassifier(class_weight='balanced')
grid = GridSearchCV(rfc_best,para,cv=5,scoring='accuracy')
grid.fit(X_train,Y_train)
prediction_best = grid.predict(X_test)
confmatrix_best = metrics.confusion_matrix(Y_test,prediction_best)
# print("The number of decision trees in forest",grid.param_grid)
print(grid.best_estimator_)
print(grid.best_score_)
print(grid.best_params_)
print("Accuracy Model RFC:",metrics.accuracy_score(Y_test, prediction_best))
print(metrics.classification_report(Y_test, prediction_best, digits=3))
sb.heatmap(pd.DataFrame(confmatrix_best), annot=True, cmap="YlGnBu" ,fmt='g')
plt.tight_layout()
plt.title('Confusion matrix RFC')
plt.ylabel('Actual label')
plt.xlabel('Predicted label')
plt.show()

from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score

predict_prob = rfc.predict_proba(X_test)
# print(predict_prob)
# print(predict_prob[:,0])
# print(predict_prob[:,1])
# print(predict_prob[:,2])

def separate(prediction):
    prediction_1vall = []
    prediction_2vall = []
    prediction_3vall = []
    for i in range(len(prediction)):
        if prediction[i]==1:
            prediction_1vall.append(1)
            prediction_2vall.append(0)
            prediction_3vall.append(0)
        if prediction[i]==2:
            prediction_1vall.append(0)
            prediction_2vall.append(1)
            prediction_3vall.append(0)
        if prediction[i]==3:
            prediction_1vall.append(0)
            prediction_2vall.append(0)
            prediction_3vall.append(1)
    return (prediction_1vall,prediction_2vall,prediction_3vall)

fpr = {}
tpr = {}   
Y_test_1, Y_test_2, Y_test_3 = separate(Y_test.to_list())
# print(Y_test_1)
# print(Y_test_2)
# print(Y_test_3)
fpr[1],tpr[1],_ = roc_curve(Y_test_1,predict_prob[:,0], pos_label=1)
fpr[2],tpr[2],_ = roc_curve(Y_test_2,predict_prob[:,1], pos_label=1)
fpr[3],tpr[3],_ = roc_curve(Y_test_3,predict_prob[:,2], pos_label=1)
# print(_)

plt.plot(fpr[1],tpr[1],linestyle = '--',label='1 vs All',color='green')
plt.plot(fpr[2],tpr[2],linestyle = '--',label='2 vs All',color='orange')
plt.plot(fpr[3],tpr[3],linestyle = '--',label='3 vs All',color='red')
plt.plot([0,1],[0,1],linestyle='--',color='blue')
plt.title("ROC Curve Random Forest Classifier")
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.legend(loc='best')
plt.show()

auc_score = metrics.roc_auc_score(Y_test,predict_prob,multi_class='ovr',average='weighted')
print("AUC for RFC Model",auc_score)

#Oversampling class2 and class3 data

# from imblearn.over_sampling import SMOTE
# from collections import Counter
# print(X)
# X_train,X_test,Y_train,Y_test=train_test_split(X,y,test_size=0.3,random_state=42,stratify=y)

# counter = Counter(Y_train)
# print("Count 1",counter)

# oversample = SMOTE()
# X_train, Y_train = oversample.fit_resample(X_train,Y_train)

# counter = Counter(Y_train)
# print("Count 2",counter)

# dtc = DecisionTreeClassifier(criterion='entropy')
# dtc.fit(X_train,Y_train)
# prediction = dtc.predict(X_test)
# confmatrix = metrics.confusion_matrix(Y_test,prediction)
# print(confmatrix)
# accuracy = metrics.accuracy_score(Y_test,prediction)
# print("Accuracy of DTC Model:",accuracy)

# classification_rep = metrics.classification_report(Y_test,prediction,digits=3)
# print("Classification Report of DTC Model:\n",classification_rep)
# print("Depth of Decision Tree",dtc.get_depth())

# sb.heatmap(pd.DataFrame(confmatrix), annot=True, cmap="YlGnBu" ,fmt='g')
# plt.tight_layout()
# plt.title("DTC Confusion Matrix")
# plt.ylabel("Actual Class")
# plt.xlabel("Predicted Class")
# plt.xticks(np.arange(3)+0.5,("Normal","Suspect","Pathological"),va='center')
# plt.yticks(np.arange(3)+0.5,("Normal","Suspect","Pathological"),va='center')
# plt.show()

# depths = []
# accuracies = []
# recall_1 = []
# recall_2 = []
# recall_3 = []
# print("Testing Set Accuracies")
# for i in range(1,20):
#     dtc_depth_i = DecisionTreeClassifier(min_samples_leaf=i)
#     dtc_depth_i.fit(X_train,Y_train)
#     prediction_i = dtc_depth_i.predict(X_test)
#     accuracy_i = metrics.accuracy_score(Y_test,prediction_i)
#     depths.append(i)
#     accuracies.append(accuracy_i)
#     classification_rep = metrics.classification_report(Y_test,prediction_i,output_dict=True,zero_division=0)
#     # print(classification_rep)
#     recall_1.append(classification_rep['1.0']['recall'])
#     recall_2.append(classification_rep['2.0']['recall'])
#     recall_3.append(classification_rep['3.0']['recall'])
#     print("Accuracy of DTC Model with depth = %d:"%i,accuracy_i)
#     # classification_rep = metrics.classification_report(Y_test,prediction_i,digits=3)
#     # print("Classification Report of DTC Model with depth = %d:\n"%i,classification_rep)
    


# fig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2)
# plt.figure(figsize=(100,100))
# fig.tight_layout()
# accuracies = np.array(accuracies)
# errors = 1 - accuracies
# ax1.plot(depths,accuracies,label='Testing Set Accuracy')
# ax1.set_ylabel("Accuracy of Model")
# ax1.set_xlabel("Depth of DT")
# ax1.set_ylim((0.6,1))

# ax2.plot(depths,recall_1)
# ax2.set_ylabel("Recall of Class 1")
# ax2.set_xlabel("Depth of DT")
# ax2.set_ylim((0.6,1))

# ax3.plot(depths,recall_2)
# ax3.set_ylabel("Recall of Class 2")
# ax3.set_xlabel("Depth of DT")
# ax3.set_ylim((0.6,1))

# ax4.plot(depths,recall_3)
# ax4.set_ylabel("Recall of Class 3")
# ax4.set_xlabel("Depth of DT")
# ax4.set_ylim((0.6,1))

# depths = []
# accuracies = []
# recall_1 = []
# recall_2 = []
# recall_3 = []
# print("Training Set Accuracies")
# for i in range(1,20):
#     dtc_depth_i = DecisionTreeClassifier(min_samples_leaf=i)
#     dtc_depth_i.fit(X_train,Y_train)
#     prediction_i = dtc_depth_i.predict(X_train)
#     accuracy_i = metrics.accuracy_score(Y_train,prediction_i)
#     depths.append(i)
#     accuracies.append(accuracy_i)
#     classification_rep = metrics.classification_report(Y_train,prediction_i,output_dict=True,zero_division=0)
    
#     recall_1.append(classification_rep['1.0']['recall'])
#     recall_2.append(classification_rep['2.0']['recall'])
#     recall_3.append(classification_rep['3.0']['recall'])
#     print("Accuracy of DTC Model with depth = %d:"%i,accuracy_i)
#     # classification_rep = metrics.classification_report(Y_train,prediction_i,digits=3)
#     # print("Classification Report of DTC Model with depth = %d:\n"%i,classification_rep)
    


# # fig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2)
# # plt.figure(figsize=(100,100))
# # fig.tight_layout()
# accuracies = np.array(accuracies)
# errors = 1 - accuracies
# ax1.plot(depths,accuracies,label='Training Set Accuracy')
# ax1.set_ylabel("Accuracy of Model")
# ax1.set_xlabel("Depth of DT")
# ax1.set_ylim((0.6,1))

# ax2.plot(depths,recall_1)
# ax2.set_ylabel("Recall of Class 1")
# ax2.set_xlabel("Depth of DT")
# ax2.set_ylim((0.6,1))

# ax3.plot(depths,recall_2)
# ax3.set_ylabel("Recall of Class 2")
# ax3.set_xlabel("Depth of DT")
# ax3.set_ylim((0.6,1))

# ax4.plot(depths,recall_3)
# ax4.set_ylabel("Recall of Class 3")
# ax4.set_xlabel("Depth of DT")
# ax4.set_ylim((0.6,1))

# # plt.legend(['Orange : Testing','Blue: Training'])
# ax1.legend(loc='best')

#SVM Model
from sklearn import svm
clf_svm = svm.SVC(kernel='rbf',C=100,probability=True)
clf_svm.fit(X_train, Y_train)
y_pred = clf_svm.predict(X_test)

from sklearn import metrics
confmatrix_svm = metrics.confusion_matrix(Y_test,y_pred)

print("Accuracy Model SVM:",metrics.accuracy_score(Y_test, y_pred))
print("Classification Report of SVM Model\n",metrics.classification_report(Y_test, y_pred, digits=3))
sb.heatmap(pd.DataFrame(confmatrix_svm), annot=True, cmap="YlGnBu" ,fmt='g')
plt.tight_layout()
plt.title('Confusion matrix SVM')
plt.ylabel('Actual label')
plt.xlabel('Predicted label')
plt.xticks(np.arange(3)+0.5,("Normal","Suspect","Pathological"),va='center')
plt.yticks(np.arange(3)+0.5,("Normal","Suspect","Pathological"),va='center')
plt.show()

from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score

predict_prob = clf_svm.predict_proba(X_test)

def separate(prediction):
    prediction_1vall = []
    prediction_2vall = []
    prediction_3vall = []
    for i in range(len(prediction)):
        if prediction[i]==1:
            prediction_1vall.append(1)
            prediction_2vall.append(0)
            prediction_3vall.append(0)
        if prediction[i]==2:
            prediction_1vall.append(0)
            prediction_2vall.append(1)
            prediction_3vall.append(0)
        if prediction[i]==3:
            prediction_1vall.append(0)
            prediction_2vall.append(0)
            prediction_3vall.append(1)
    return (prediction_1vall,prediction_2vall,prediction_3vall)

fpr = {}
tpr = {}   
Y_test_1, Y_test_2, Y_test_3 = separate(Y_test.to_list())

fpr[1],tpr[1],_ = roc_curve(Y_test_1,predict_prob[:,0], pos_label=1)
fpr[2],tpr[2],_ = roc_curve(Y_test_2,predict_prob[:,1], pos_label=1)
fpr[3],tpr[3],_ = roc_curve(Y_test_3,predict_prob[:,2], pos_label=1)

plt.plot(fpr[1],tpr[1],linestyle = '--',label='1 vs All',color='green')
plt.plot(fpr[2],tpr[2],linestyle = '--',label='2 vs All',color='orange')
plt.plot(fpr[3],tpr[3],linestyle = '--',label='3 vs All',color='red')
plt.plot([0,1],[0,1],linestyle='--',color='blue')
plt.title("ROC Curve for SVM Model")
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.legend(loc='best')
plt.show()

auc_score = metrics.roc_auc_score(Y_test,predict_prob,multi_class='ovr',average='weighted')
print("AUC for SVM Model",auc_score)



#MLP classifier model
from sklearn.neural_network import MLPClassifier
from sklearn.datasets import make_classification
clf = MLPClassifier(hidden_layer_sizes=(16,32,64,128,256), activation='relu',random_state=1, max_iter=1000 ).fit(X_train, Y_train)
y_pred=clf.predict(X_test)

from sklearn import metrics
confmatrix_MLP = metrics.confusion_matrix(Y_test,y_pred)

print("Accuracy Model MLP:",metrics.accuracy_score(Y_test, y_pred))
print("Classification Report of MLP Model\n",metrics.classification_report(Y_test, y_pred, digits=3))
sb.heatmap(pd.DataFrame(confmatrix_MLP), annot=True, cmap="YlGnBu" ,fmt='g')
plt.tight_layout()
plt.title('Confusion matrix MLP')
plt.ylabel('Actual label')
plt.xlabel('Predicted label')
plt.xticks(np.arange(3)+0.5,("Normal","Suspect","Pathological"),va='center')
plt.yticks(np.arange(3)+0.5,("Normal","Suspect","Pathological"),va='center')
plt.show()

predict_prob = clf.predict_proba(X_test)

def separate(prediction):
    prediction_1vall = []
    prediction_2vall = []
    prediction_3vall = []
    for i in range(len(prediction)):
        if prediction[i]==1:
            prediction_1vall.append(1)
            prediction_2vall.append(0)
            prediction_3vall.append(0)
        if prediction[i]==2:
            prediction_1vall.append(0)
            prediction_2vall.append(1)
            prediction_3vall.append(0)
        if prediction[i]==3:
            prediction_1vall.append(0)
            prediction_2vall.append(0)
            prediction_3vall.append(1)
    return (prediction_1vall,prediction_2vall,prediction_3vall)

fpr = {}
tpr = {}   
Y_test_1, Y_test_2, Y_test_3 = separate(Y_test.to_list())

fpr[1],tpr[1],_ = roc_curve(Y_test_1,predict_prob[:,0], pos_label=1)
fpr[2],tpr[2],_ = roc_curve(Y_test_2,predict_prob[:,1], pos_label=1)
fpr[3],tpr[3],_ = roc_curve(Y_test_3,predict_prob[:,2], pos_label=1)

plt.plot(fpr[1],tpr[1],linestyle = '--',label='1 vs All',color='green')
plt.plot(fpr[2],tpr[2],linestyle = '--',label='2 vs All',color='orange')
plt.plot(fpr[3],tpr[3],linestyle = '--',label='3 vs All',color='red')
plt.plot([0,1],[0,1],linestyle='--',color='blue')
plt.title("ROC Curve for MLP Model")
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.legend(loc='best')
plt.show()

auc_score = metrics.roc_auc_score(Y_test,predict_prob,multi_class='ovr',average='weighted')
print("AUC for MLP Model",auc_score)
